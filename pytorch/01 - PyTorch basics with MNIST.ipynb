{"cells": [{"metadata": {"trusted": true, "ExecuteTime": {"start_time": "2018-06-16T22:30:25.586241Z", "end_time": "2018-06-16T22:30:25.593723Z"}}, "cell_type": "code", "source": "import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm, tnrange", "execution_count": 65, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## PyTorch Overview:\n\nPyTorch is a deep learning framework for fast, flexible experimentation.\n\nPyTorch consists of 4 main packages:\n\n- torch: a general purpose array library similar to Numpy that can do computations on GPU when the tensor type is cast to (torch.cuda.TensorFloat)\n- torch.autograd: a package for building a computational graph and automatically obtaining gradients\n- torch.nn: a neural net library with common layers and cost functions\n- torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Get Data"}, {"metadata": {}, "cell_type": "markdown", "source": "### Dataset\n\nMNIST dataset is part of PyTorch and can be downloaded. We'll learn to create custom datasets later. A dataset in Pytorch is a subclass of torch.utils.data.Dataset thas has methods \\__getitem__ and \\__len__ methods implemented."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_ds = datasets.MNIST('data', train=True, download=True, \n                       transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ]))\ntest_ds = datasets.MNIST('data', train=False, download=True, \n                       transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ]))", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# train_ds[0] is a tuple with an image (x) and a class (y)\nx, y = train_ds[0]\nprint(x.shape)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "torch.Size([1, 28, 28])\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### DataLoader\n\nData loader combines a dataset and a sampler, and provides an iterator over the dataset. The data loader divides the data in mini batches. This is particularly important when working with large dataset that cannot be hold in memory."}, {"metadata": {"trusted": true, "ExecuteTime": {"start_time": "2018-06-16T21:20:03.318272Z", "end_time": "2018-06-16T21:20:03.321576Z"}}, "cell_type": "code", "source": "batch_size = 64\n# Train data should be shuffled between epochs\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n# for test we use shuffle=False\ntest_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**How does a batch look like?**"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:21:18.167633Z", "end_time": "2018-06-16T21:21:18.187443Z"}, "trusted": true}, "cell_type": "code", "source": "train_dl = iter(train_loader)\nx, y = next(train_dl)\nx.shape, y.shape", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "**What's in the tensors?**"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:22:00.323680Z", "end_time": "2018-06-16T21:22:00.326894Z"}, "trusted": true}, "cell_type": "code", "source": "def show(img, title=None):\n    plt.imshow(img, interpolation='none', cmap=\"gray\")\n    if title is not None: plt.title(title)", "execution_count": 9, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:23:36.180954Z", "end_time": "2018-06-16T21:23:36.337520Z"}, "trusted": true}, "cell_type": "code", "source": "X = x.numpy()\nY = y.numpy()\nshow(X[1][0], Y[1])", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "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\n"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Create a Layer"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:40:49.713012Z", "end_time": "2018-06-16T21:40:49.716352Z"}, "trusted": true}, "cell_type": "code", "source": "# Linear layer (characteristic of feed forward networks), transforms n*D data into n*M where,\n# n is the number of observations\n# D is the number of features\n# M is the number of transformed features\nD = 5\nM = 3\nlin_layer = nn.Linear(D, M)", "execution_count": 26, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "When a layer is intantiated, parameters of the layer (weights and biases) are randomly initialized"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:36:42.922762Z", "end_time": "2018-06-16T21:36:42.928534Z"}, "trusted": true}, "cell_type": "code", "source": "[p for p in lin_layer.parameters()]", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "[Parameter containing:\n tensor([[ 0.4358, -0.3806, -0.2367,  0.4000,  0.1718],\n         [ 0.3792, -0.3616, -0.2295, -0.1537, -0.1973],\n         [-0.0468,  0.1025, -0.2533,  0.3260, -0.3701]]), Parameter containing:\n tensor([-0.2213, -0.1573, -0.4115])]"}, "metadata": {}}]}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:37:19.386983Z", "end_time": "2018-06-16T21:37:19.392640Z"}, "trusted": true}, "cell_type": "code", "source": "n = 10\n# Create random input\nA = torch.randn(n, D)\nA", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "tensor([[ 0.8302,  0.3401,  0.2838, -0.0232,  0.9266],\n        [ 0.6903, -0.9846, -1.6355, -1.1105,  3.0589],\n        [ 0.5845,  0.1767,  1.4728,  0.7216, -0.8400],\n        [ 1.7570,  0.4217, -0.4522,  0.2240, -1.6039],\n        [ 1.9590, -0.4269,  0.4313,  1.3806,  1.4492],\n        [ 0.3536,  1.5563, -1.6681, -0.8075, -1.2534],\n        [ 1.3373, -1.3712, -2.6185, -1.4181, -0.4229],\n        [-0.1159, -1.7881, -0.8945,  1.6791, -1.4910],\n        [ 0.3747,  1.8120,  1.3073,  1.1465, -0.1520],\n        [ 0.8568,  0.0198,  1.5436,  0.7328, -0.2708]])"}, "metadata": {}}]}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:37:21.747562Z", "end_time": "2018-06-16T21:37:21.752550Z"}, "trusted": true}, "cell_type": "code", "source": "lin_layer(A)", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "tensor([[ 0.0938, -0.2098, -0.8379],\n        [ 0.9228,  0.4032, -1.6244],\n        [-0.2382, -0.2828, -0.2478],\n        [ 0.3049,  0.7422,  0.3305],\n        [ 1.4941,  0.1428, -0.7425],\n        [-0.8030,  0.1683,  0.3546],\n        [ 0.8634,  1.7480, -0.2571],\n        [ 1.0359,  0.6865,  0.7365],\n        [-0.6246, -1.1167, -0.1446],\n        [ 0.0257, -0.2531, -0.5016]])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Create a Model"}, {"metadata": {}, "cell_type": "markdown", "source": "### nn.Sequential"}, {"metadata": {}, "cell_type": "markdown", "source": "To create a model with nn.Sequential you provide a list of layers. For example, the following model defines a 2-layer neural network with $784$ input features ($D = 784$), $300$ hidden layers ($M=300$) and $10$ outputs. This model uses Relu activation funtion and no final activation."}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:40:24.581770Z", "end_time": "2018-06-16T21:40:24.588764Z"}, "trusted": true}, "cell_type": "code", "source": "net = nn.Sequential(nn.Linear(784, 300),\n                    nn.ReLU(),\n                    nn.Linear(300, 10))", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### nn.Module"}, {"metadata": {}, "cell_type": "markdown", "source": "A more flexible way to define models in pytorch is as a subclass of nn.Module. In the __init__ method we define all layers that will be used later. In the forward method, we define the actual model using the already defined layers."}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:42:23.134167Z", "end_time": "2018-06-16T21:42:23.142005Z"}, "trusted": true}, "cell_type": "code", "source": "class Net(nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear modules and assign them as\n        member variables (self).\n        \"\"\"\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(784, 300)\n        self.linear2 = nn.Linear(300, 10)\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.linear2(x)\n        return x\n\nnet = Net()", "execution_count": 27, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:42:41.955125Z", "end_time": "2018-06-16T21:42:41.957614Z"}, "trusted": true}, "cell_type": "code", "source": "# this line prints all parameters\n# [p for p in net.parameters()]", "execution_count": 28, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### optim (Optimizer)"}, {"metadata": {}, "cell_type": "markdown", "source": "torch.optim provides implementations of commonly used optimization algorithms (such us gradient descent and momentum). You need to specify the algorithm you want to use. Adam is a popular algorithm. You also specify the parameters you want to optimize and the learning rate. If you want to use $L_2$ regularization you can specify the weight decay."}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T21:45:08.245432Z", "end_time": "2018-06-16T21:45:08.248467Z"}, "trusted": true}, "cell_type": "code", "source": "learning_rate = 0.01\noptimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.01)", "execution_count": 29, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Training Loop"}, {"metadata": {}, "cell_type": "markdown", "source": "Each epoch of the training has to go through entire dataset (multiple mini-batches) also called forward pass and gradients should be propogated backwards (BackProp). The purpose of the training loop is to set model to train and process the data using dataloaders created and update the parameters."}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:15:56.459267Z", "end_time": "2018-06-16T22:15:56.464018Z"}, "trusted": true}, "cell_type": "code", "source": "def train_model(train_loader, model, optimizer):\n    # set model to training state\n    model.train()\n    sum_loss = 0.0 # Initialize loss to zero\n    total_samples = 0 # Initialize data processed by the model to zero\n    \n    for images, labels in train_loader:\n        batch = images.shape[0] # size of the batch\n        images = images.view(-1, 28*28) # flattening the images\n        pred = model(images) # forward pass, make predictins\n        loss = F.cross_entropy(pred, labels) # compute loss\n        optimizer.zero_grad() # zero the gradient buffer\n        loss.backward() # backprop gradients\n        optimizer.step() # update parameters\n        sum_loss += batch * loss.item() # not all batches are of same size\n        total_samples += batch\n    \n    train_loss = sum_loss/total_samples # compute mean loss\n    return train_loss", "execution_count": 51, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Model Evaluation"}, {"metadata": {}, "cell_type": "markdown", "source": "Given a model and data (in the form of a dataloader), we evaluate the model performance."}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:06:55.767551Z", "end_time": "2018-06-16T22:06:55.772015Z"}, "trusted": true}, "cell_type": "code", "source": "def model_eval(model, data_loader):\n    model.eval() # set the model to evaluation mode\n    correct = 0 # Initialize correct predictions \n    sum_loss = 0.0 # Initialize total loss\n    total = 0 # Initialize samples processed by the model to zero\n    for images, labels in data_loader:\n        batch = labels.size(0) # Equivalent to labels.shape[0]\n        images = images.view(-1, 28*28)\n        pred = model(images)\n        loss = F.cross_entropy(pred, labels)\n        sum_loss += batch*loss.item()\n        total += batch\n        _, pred = torch.max(pred.data, 1) # computes a hard prediction\n        correct += pred.eq(labels.data).sum().item()\n    return 100 * correct / total, sum_loss/ total", "execution_count": 35, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Training the Model"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:16:03.793047Z", "end_time": "2018-06-16T22:16:03.798755Z"}, "trusted": true}, "cell_type": "code", "source": "net = Net()\nlearning_rate = 0.01\noptimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.0)", "execution_count": 52, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:16:04.159583Z", "end_time": "2018-06-16T22:17:47.272945Z"}, "trusted": true}, "cell_type": "code", "source": "epochs = 10\nfor i in tnrange(epochs, desc='Epoch'):\n    train_loss = train_model(train_loader, net, optimizer)\n    print(\"train loss \", train_loss)\n    acc, loss = model_eval(net, test_loader)\n    print(\"accuracy and test loss \", acc, loss)", "execution_count": 53, "outputs": [{"output_type": "display_data", "data": {"text/plain": "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "970ac434136748d9b95a3cabebb05de3"}}, "metadata": {}}, {"output_type": "stream", "text": "train loss  0.28557449571192267\naccuracy and test loss  93.91 0.23288310603946447\ntrain loss  0.21190326106796661\naccuracy and test loss  91.75 0.3497096668928862\ntrain loss  0.19250318566312394\naccuracy and test loss  94.52 0.2249447972588241\ntrain loss  0.1797704157114029\naccuracy and test loss  94.59 0.25306223648935555\ntrain loss  0.17958513912657897\naccuracy and test loss  94.92 0.22026115838708357\ntrain loss  0.16937018661759795\naccuracy and test loss  95.48 0.24710467757806182\ntrain loss  0.16845348128850263\naccuracy and test loss  94.97 0.2388424200611189\ntrain loss  0.15729533572234214\naccuracy and test loss  95.74 0.2231523359847488\ntrain loss  0.15582410916984082\naccuracy and test loss  95.36 0.26285289277319096\ntrain loss  0.15476816681635877\naccuracy and test loss  95.47 0.2608021271620528\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Create 3 layer network and decrease learning rate after 5 epochs"}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:30:07.004085Z", "end_time": "2018-06-16T22:30:07.008293Z"}, "trusted": true}, "cell_type": "code", "source": "class Net3Layer(nn.Module):\n    def __init__(self):\n        super(Net3Layer, self).__init__()\n        self.layer1 = nn.Linear(784, 500)\n        self.layer2 = nn.Linear(500, 250)\n        self.layer3 = nn.Linear(250,10)\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = F.relu(x)\n        x = self.layer2(x)\n        x = F.relu(x)\n        x = self.layer3(x)\n        \n        return x", "execution_count": 62, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:30:07.366506Z", "end_time": "2018-06-16T22:30:07.374306Z"}, "trusted": true}, "cell_type": "code", "source": "net3L = Net3Layer()\nlearning_rate = 0.01\noptimizer3L = optim.Adam(net3L.parameters(), lr=learning_rate)", "execution_count": 63, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2018-06-16T22:31:08.219663Z", "end_time": "2018-06-16T22:33:53.931385Z"}, "trusted": true}, "cell_type": "code", "source": "n_epochs = 10\nfor i in tnrange(n_epochs, desc='Epoch'):\n    if i==5:\n        for p in optimizer3L.param_groups:\n            p['lr'] = 0.001\n    train_loss = train_model(train_loader, net3L, optimizer3L)\n    tqdm.write(f'Train loss: {train_loss:{.4}}')\n    acc, test_loss = model_eval(net3L, test_loader)\n    tqdm.write(f'Test accuracy: {acc:{2.4}} and loss: {test_loss:{.4}}')", "execution_count": 67, "outputs": [{"output_type": "display_data", "data": {"text/plain": "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "450c7568859b4cd4a1c1f37cde139e90"}}, "metadata": {}}, {"output_type": "stream", "text": "Train loss: 0.1738\nTest accuracy: 95.1 and loss: 0.2096\nTrain loss: 0.1612\nTest accuracy: 95.85 and loss: 0.1998\nTrain loss: 0.148\nTest accuracy: 95.21 and loss: 0.2539\nTrain loss: 0.149\nTest accuracy: 95.82 and loss: 0.1999\nTrain loss: 0.1451\nTest accuracy: 96.32 and loss: 0.2002\nTrain loss: 0.07873\nTest accuracy: 97.23 and loss: 0.1305\nTrain loss: 0.0601\nTest accuracy: 97.31 and loss: 0.1313\nTrain loss: 0.05337\nTest accuracy: 97.36 and loss: 0.1285\nTrain loss: 0.04829\nTest accuracy: 97.29 and loss: 0.1329\nTrain loss: 0.04465\nTest accuracy: 97.28 and loss: 0.135\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "conda-env-pytorch_p36-py", "display_name": "Python [conda env:pytorch_p36]", "language": "python"}, "language_info": {"name": "python", "version": "3.6.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}